{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "- Enable wider audiences beyond “Big Data” engineers to leverage the power of distributed processing\n",
    "- Inspired by data frames in R and Python (Pandas)\n",
    "- Designed from the ground-up to support modern big\n",
    "data and data science applications\n",
    "- Extension to the existing RDD API\n",
    "\n",
    "## References\n",
    "- [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [Introduction to DataFrames - Python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)\n",
    "- [PySpark Cheat Sheet: Spark DataFrames in Python](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames are :\n",
    "- The preferred abstraction in Spark\n",
    "- Strongly typed collection of distributed elements \n",
    "- Built on Resilient Distributed Datasets (RDD)\n",
    "- Immutable once constructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Dataframes you can :\n",
    "- Track lineage information to efficiently recompute lost data \n",
    "- Enable operations on collection of elements in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You construct DataFrames\n",
    "- by parallelizing existing collections (e.g., Pandas DataFrames) \n",
    "- by transforming an existing DataFrames\n",
    "- from files in HDFS or any other storage system (e.g., Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "- Ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster\n",
    "- Support for a wide array of data formats and storage systems\n",
    "- Seamless integration with all big data tooling and infrastructure via Spark\n",
    "- APIs for Python, Java, Scala, and R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames versus RDDs\n",
    "- Nice API for new users familiar with data frames in other programming languages.\n",
    "- For existing Spark users, the API will make Spark easier to program than using RDDs\n",
    "- For both sets of users, DataFrames will improve performance through intelligent optimizations and code-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Spark shell:**\n",
    "\n",
    "~~~ bash\n",
    "pyspark\n",
    "~~~\n",
    "\n",
    "Output similar to the following will be displayed, followed by a `>>>` REPL prompt:\n",
    "\n",
    "~~~\n",
    "Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)\n",
    "[GCC 7.2.0] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "2018-09-18 17:13:13 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    "~~~\n",
    "\n",
    "Read data and convert to Dataset\n",
    "\n",
    "~~~ py\n",
    "df = sqlContext.read.csv(\"data/AnaDo_JeuDonnees_TemperatFrance.csv\", sep=';', header=True)\n",
    "~~~\n",
    "\n",
    "~~~\n",
    ">>> df.show()\n",
    "+---+--------------------+------------+------+------------+--------+---+-----------+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+----+------+\n",
    "|        _c0|Janv|Fevr|Mars|Avri| Mai|Juin|juil|Aout|Sept|Octo|Nove|Dece| Lati| Long| Moye|Ampl|Region|\n",
    "+-----------+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+----+------+\n",
    "|   Bordeaux| 5.6| 6.6|10.3|12.8|15.8|19.3|20.9|  21|18.6|13.8| 9.1| 6.2| 44.5|-0.34|13.33|15.4|    SO|\n",
    "|      Brest| 6.1| 5.8| 7.8| 9.2|11.6|14.4|15.6|  16|14.7|  12|   9|   7|48.24|-4.29|10.77|10.2|    NO|\n",
    "|   Clermont| 2.6| 3.7| 7.5|10.3|13.8|17.3|19.4|19.1|16.2|11.2| 6.6| 3.6|45.47| 3.05|10.94|16.8|    SE|\n",
    "|   Grenoble| 1.5| 3.2| 7.7|10.6|14.5|17.8|20.1|19.5|16.7|11.4| 6.5| 2.3| 45.1| 5.43|10.98|18.6|    SE|\n",
    "|      Lille| 2.4| 2.9|   6| 8.9|12.4|15.3|17.1|17.1|14.7|10.4| 6.1| 3.5|50.38| 3.04| 9.73|14.7|    NE|\n",
    "|       Lyon| 2.1| 3.3| 7.7|10.9|14.9|18.5|20.7|20.1|16.9|11.4| 6.7| 3.1|45.45| 4.51|11.36|18.6|    SE|\n",
    "\n",
    "only showing top 20 rows\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations, Actions, Laziness\n",
    "\n",
    "Like RDDs, DataFrames are lazy. Transformations contribute to the query plan, but they don't execute anything.\n",
    "Actions cause the execution of the query.\n",
    "\n",
    "### Transformation examples\n",
    "- filter\n",
    "- select\n",
    "- drop\n",
    "- intersect \n",
    "- join\n",
    "### Action examples\n",
    "- count \n",
    "- collect \n",
    "- show \n",
    "- head\n",
    "- take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4MB)\n",
      "\u001b[K    18% |██████                          | 39.9MB 5.0MB/s eta 0:00:355 0% |▏                               | 778kB 1.1MB/s eta 0:03:18"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "# The following three lines are not necessary\n",
    "# in the pyspark shell\n",
    "conf = SparkConf().setAppName(\"people\").setMaster(\"local[*]\") \n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.json(\"../data/employees.json\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Inference\n",
    "\n",
    "In this exercise, let's explore schema inference. We're going to be using a file called `AnaDo_JeuDonnees_TemperatFrance.csv`. The data is structured, but it has no self-describing schema. And, it's not JSON, so Spark can't infer the schema automatically. Let's create an RDD and look at the first few rows of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";Janv;F�vr;Mars;Avri;Mai;Juin;juil;Ao�t;Sept;Octo;Nove;D�ce;Lati;Long;Moye;Ampl;R�gion\n",
      "Bordeaux;5.6;6.6;10.3;12.8;15.8;19.3;20.9;21;18.6;13.8;9.1;6.2;44.5;-0.34;13.33;15.4;SO\n",
      "Brest;6.1;5.8;7.8;9.2;11.6;14.4;15.6;16;14.7;12;9;7;48.24;-4.29;10.77;10.2;NO\n",
      "Clermont;2.6;3.7;7.5;10.3;13.8;17.3;19.4;19.1;16.2;11.2;6.6;3.6;45.47;3.05;10.94;16.8;SE\n",
      "Grenoble;1.5;3.2;7.7;10.6;14.5;17.8;20.1;19.5;16.7;11.4;6.5;2.3;45.1;5.43;10.98;18.6;SE\n",
      "Lille;2.4;2.9;6;8.9;12.4;15.3;17.1;17.1;14.7;10.4;6.1;3.5;50.38;3.04;9.73;14.7;NE\n",
      "Lyon;2.1;3.3;7.7;10.9;14.9;18.5;20.7;20.1;16.9;11.4;6.7;3.1;45.45;4.51;11.36;18.6;SE\n",
      "Marseille;5.5;6.6;10;13;16.8;20.8;23.3;22.8;19.9;15;10.2;6.9;43.18;5.24;14.23;17.8;SE\n",
      "Montpellier;5.6;6.7;9.9;12.8;16.2;20.1;22.7;22.3;19.3;14.6;10;6.5;43.36;3.53;13.89;17.1;SE\n",
      "Nantes;5;5.3;8.4;10.8;13.9;17.2;18.8;18.6;16.4;12.2;8.2;5.5;47.13;-1.33;11.69;13.8;NO\n"
     ]
    }
   ],
   "source": [
    "#File adresse from the spark context\n",
    "rdd = sc.textFile(\"../data/AnaDo_JeuDonnees_TemperatFrance.csv\")\n",
    "for line in rdd.take(10):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "You can look at the <a href=\"http://spark.apache.org/docs/2.3.1/api/python/index.html\" target=\"_blank\">DataFrames API documentation</a> \n",
    "\n",
    "Let's take a look to file `\n",
    "AnaDo_JeuDonnees_TemperatFrance.csv`. Each line consists \n",
    "of the same information about a person:\n",
    "\n",
    "* city\n",
    "* tempJan - tempDec\n",
    "* latitude\n",
    "* logitude\n",
    "* moyenne \n",
    "* ampl\n",
    "* region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "rdd = sc.textFile(\"../data/AnaDo_JeuDonnees_TemperatFrance.csv\")\n",
    "\n",
    "City = namedtuple('City',['Name','Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jui', 'Aug',\n",
    "                             'Sep', 'Oct', 'Nov', 'Dec', 'Lat', 'Long',\n",
    "                               'Moy', 'Ampl', 'Region'])\n",
    "            \n",
    "def map_to_city(line):\n",
    "  cols = line.split(\";\")\n",
    "  return City(Name       = cols[0],\n",
    "              Jan        = cols[1],\n",
    "              Feb        = cols[2],\n",
    "              Mar        = cols[3],\n",
    "              Apr        = cols[4],\n",
    "              May        = cols[5],\n",
    "              Jun        = cols[6],\n",
    "              Jui        = cols[7],\n",
    "              Aug        = cols[8],\n",
    "              Sep        = cols[9],\n",
    "              Oct        = cols[10],\n",
    "              Nov        = cols[11],\n",
    "              Dec        = cols[12],\n",
    "              Lat        = cols[13],\n",
    "              Long       = cols[14],\n",
    "              Moy        = cols[15], \n",
    "              Ampl       = cols[16],\n",
    "              Region     = cols[17])\n",
    "    \n",
    "city_rdd = rdd.map(map_to_city)\n",
    "df = city_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+----+------+\n",
      "|       Name| Jan| Feb| Mar| Apr| May| Jun| Jui| Aug| Sep| Oct| Nov| Dec|  Lat| Long|  Moy|Ampl|Region|\n",
      "+-----------+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+----+------+\n",
      "|           |Janv|F�vr|Mars|Avri| Mai|Juin|juil|Ao�t|Sept|Octo|Nove|D�ce| Lati| Long| Moye|Ampl|R�gion|\n",
      "|   Bordeaux| 5.6| 6.6|10.3|12.8|15.8|19.3|20.9|  21|18.6|13.8| 9.1| 6.2| 44.5|-0.34|13.33|15.4|    SO|\n",
      "|      Brest| 6.1| 5.8| 7.8| 9.2|11.6|14.4|15.6|  16|14.7|  12|   9|   7|48.24|-4.29|10.77|10.2|    NO|\n",
      "|   Clermont| 2.6| 3.7| 7.5|10.3|13.8|17.3|19.4|19.1|16.2|11.2| 6.6| 3.6|45.47| 3.05|10.94|16.8|    SE|\n",
      "|   Grenoble| 1.5| 3.2| 7.7|10.6|14.5|17.8|20.1|19.5|16.7|11.4| 6.5| 2.3| 45.1| 5.43|10.98|18.6|    SE|\n",
      "|      Lille| 2.4| 2.9|   6| 8.9|12.4|15.3|17.1|17.1|14.7|10.4| 6.1| 3.5|50.38| 3.04| 9.73|14.7|    NE|\n",
      "|       Lyon| 2.1| 3.3| 7.7|10.9|14.9|18.5|20.7|20.1|16.9|11.4| 6.7| 3.1|45.45| 4.51|11.36|18.6|    SE|\n",
      "|  Marseille| 5.5| 6.6|  10|  13|16.8|20.8|23.3|22.8|19.9|  15|10.2| 6.9|43.18| 5.24|14.23|17.8|    SE|\n",
      "|Montpellier| 5.6| 6.7| 9.9|12.8|16.2|20.1|22.7|22.3|19.3|14.6|  10| 6.5|43.36| 3.53|13.89|17.1|    SE|\n",
      "|     Nantes|   5| 5.3| 8.4|10.8|13.9|17.2|18.8|18.6|16.4|12.2| 8.2| 5.5|47.13|-1.33|11.69|13.8|    NO|\n",
      "|       Nice| 7.5| 8.5|10.8|13.3|16.7|20.1|22.7|22.5|20.3|  16|11.5| 8.2|43.42| 7.15|14.84|15.2|    SE|\n",
      "|      Paris| 3.4| 4.1| 7.6|10.7|14.3|17.5|19.1|18.7|  16|11.4| 7.1| 4.3|48.52|  2.2|11.18|15.7|    NE|\n",
      "|     Rennes| 4.8| 5.3| 7.9|10.1|13.1|16.2|17.9|17.8|15.7|11.6| 7.8| 5.4|48.05|-1.41|11.13|13.1|    NO|\n",
      "| Strasbourg| 0.4| 1.5| 5.6| 9.8|  14|17.2|  19|18.3|15.1| 9.5| 4.9| 1.3|48.35| 7.45| 9.72|18.6|    NE|\n",
      "|   Toulouse| 4.7| 5.6| 9.2|11.6|14.9|18.7|20.9|20.9|18.3|13.3| 8.6| 5.5|43.36| 1.26|12.68|16.2|    SO|\n",
      "|      Vichy| 2.4| 3.4| 7.1| 9.9|13.6|17.1|19.3|18.8|  16|  11| 6.6| 3.4|46.08| 3.26|10.72|16.9|    SE|\n",
      "+-----------+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Jan: string (nullable = true)\n",
      " |-- Feb: string (nullable = true)\n",
      " |-- Mar: string (nullable = true)\n",
      " |-- Apr: string (nullable = true)\n",
      " |-- May: string (nullable = true)\n",
      " |-- Jun: string (nullable = true)\n",
      " |-- Jui: string (nullable = true)\n",
      " |-- Aug: string (nullable = true)\n",
      " |-- Sep: string (nullable = true)\n",
      " |-- Oct: string (nullable = true)\n",
      " |-- Nov: string (nullable = true)\n",
      " |-- Dec: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Moy: string (nullable = true)\n",
      " |-- Ampl: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Jan: string, Feb: string, Mar: string, Apr: string, May: string, Jun: string, Jui: string, Aug: string, Sep: string, Oct: string, Nov: string, Dec: string, Lat: string, Long: string, Moy: string, Ampl: string, Region: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Moy: string, Region: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df[\"Name\"], df[\"Moy\"], df[\"Region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------+\n",
      "|       Name|  Moy|Region|\n",
      "+-----------+-----+------+\n",
      "|           | Moye|R�gion|\n",
      "|   Bordeaux|13.33|    SO|\n",
      "|      Brest|10.77|    NO|\n",
      "|   Clermont|10.94|    SE|\n",
      "|   Grenoble|10.98|    SE|\n",
      "|      Lille| 9.73|    NE|\n",
      "|       Lyon|11.36|    SE|\n",
      "|  Marseille|14.23|    SE|\n",
      "|Montpellier|13.89|    SE|\n",
      "|     Nantes|11.69|    NO|\n",
      "|       Nice|14.84|    SE|\n",
      "|      Paris|11.18|    NE|\n",
      "|     Rennes|11.13|    NO|\n",
      "| Strasbourg| 9.72|    NE|\n",
      "|   Toulouse|12.68|    SO|\n",
      "|      Vichy|10.72|    SE|\n",
      "+-----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"Name\"], df[\"Moy\"], df[\"Region\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+---+----+----+----+----+----+----+----+---+---+-----+----+-----+----+------+\n",
      "|      Name|Jan|Feb|Mar| Apr| May| Jun| Jui| Aug| Sep| Oct|Nov|Dec|  Lat|Long|  Moy|Ampl|Region|\n",
      "+----------+---+---+---+----+----+----+----+----+----+----+---+---+-----+----+-----+----+------+\n",
      "|     Lille|2.4|2.9|  6| 8.9|12.4|15.3|17.1|17.1|14.7|10.4|6.1|3.5|50.38|3.04| 9.73|14.7|    NE|\n",
      "|     Paris|3.4|4.1|7.6|10.7|14.3|17.5|19.1|18.7|  16|11.4|7.1|4.3|48.52| 2.2|11.18|15.7|    NE|\n",
      "|Strasbourg|0.4|1.5|5.6| 9.8|  14|17.2|  19|18.3|15.1| 9.5|4.9|1.3|48.35|7.45| 9.72|18.6|    NE|\n",
      "+----------+---+---+---+----+----+----+----+----+----+----+---+---+-----+----+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"Region\"] == \"NE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter + select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|substring(Name, 1, 1)|  Moy|\n",
      "+---------------------+-----+\n",
      "|                    L| 9.73|\n",
      "|                    P|11.18|\n",
      "|                    S| 9.72|\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.filter(df[\"Region\"] == \"NE\").select(df['Name'].substr(1,1),df['Moy'])\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      Name|  Moy|\n",
      "+----------+-----+\n",
      "|     Paris|11.18|\n",
      "|Strasbourg| 9.72|\n",
      "|     Lille| 9.73|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.filter(df[\"Region\"] == \"NE\")\n",
    "   .select(df['Name'],df['Moy'])\n",
    "   .orderBy(\"Moy\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fd514bb1780>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(df[\"Region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Region|count|\n",
      "+------+-----+\n",
      "|    SO|    2|\n",
      "|    NE|    3|\n",
      "|R�gion|    1|\n",
      "|    SE|    7|\n",
      "|    NO|    3|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(df[\"Region\"]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: Don't confuse GroupedData.count() with DataFrame.count(). GroupedData.count() is not an action. DataFrame.count() is an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df[\"Region\"] == \"NE\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      name|\n",
      "+----------+\n",
      "|     Lille|\n",
      "|     Paris|\n",
      "|Strasbourg|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"Region\"] == \"NE\").select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Region|count|\n",
      "+------+-----+\n",
      "|    SO|    2|\n",
      "|    NE|    3|\n",
      "|R�gion|    1|\n",
      "|    SE|    7|\n",
      "|    NO|    3|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df[\"Region\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       Name|  Moy|\n",
      "+-----------+-----+\n",
      "|           | Moye|\n",
      "|   Bordeaux|13.33|\n",
      "|      Brest|10.77|\n",
      "|   Clermont|10.94|\n",
      "|   Grenoble|10.98|\n",
      "|      Lille| 9.73|\n",
      "|       Lyon|11.36|\n",
      "|  Marseille|14.23|\n",
      "|Montpellier|13.89|\n",
      "|     Nantes|11.69|\n",
      "|       Nice|14.84|\n",
      "|      Paris|11.18|\n",
      "|     Rennes|11.13|\n",
      "| Strasbourg| 9.72|\n",
      "|   Toulouse|12.68|\n",
      "|      Vichy|10.72|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['Name'],df['Moy']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- How many cities from the South-East region ?\n",
    "- How many city with a mean temperature higher than 10 degrees ?\n",
    "- How many cities in the south regions (south-east + south-west) ?\n",
    "- What is the ratio of the north-est cities / all the cities ?\n",
    "- Which region contains most cities ?\n",
    "- List number of city for each first letter of a city name ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
